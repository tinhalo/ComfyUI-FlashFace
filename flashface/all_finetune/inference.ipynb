{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: x:\\dev\\ComfyUI-FlashFace\\flashface\\all_finetune\n",
      "Package directory: x:\\dev\\ComfyUI-FlashFace\n",
      "Python path: ['x:\\\\dev\\\\ComfyUI-FlashFace', 'C:\\\\Python\\\\Python13\\\\python313.zip', 'C:\\\\Python\\\\Python13\\\\DLLs']\n",
      "Contents of package_dir: ['.git', '.github', '.gitignore', '.venv', 'cache', 'docs', 'example_workflows', 'figs', 'flashface', 'install_dependencies.py', 'ldm', 'LICENSE', 'nodes', 'pyproject.toml', 'readme.md', 'requirements-comfy.txt', 'requirements.txt', 'setup.bat', 'setup.sh', '__init__.py']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "x:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "x:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "x:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "x:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Fix path setup for Windows - use os.path operations instead of string replacement\n",
    "current_dir = os.path.abspath('')\n",
    "# Go up two levels from flashface/all_finetune to reach the root\n",
    "package_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "sys.path.insert(0, package_dir)\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Package directory: {package_dir}\")\n",
    "print(f\"Python path: {sys.path[:3]}\")\n",
    "print(f\"Contents of package_dir: {os.listdir(package_dir) if os.path.exists(package_dir) else 'Directory not found'}\")\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from config import cfg\n",
    "from models import sd_v1_ref_unet\n",
    "from ops.context_diffusion import ContextGaussianDiffusion\n",
    "from ldm import data, models, ops\n",
    "from ldm.models.vae import sd_v1_vae\n",
    "from ldm.utils import load_model_weights\n",
    "from utils import Compose, PadToSquare, get_padding, seed_everything\n",
    "from ldm.models.retinaface import retinaface as retinaface_func, crop_face\n",
    "\n",
    "# Import functions from demo_gradio.py\n",
    "from demo_gradio import generate, encode_text\n",
    "from enhanced_transforms import EnhancedPadToSquare, create_face_transforms\n",
    "\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "model initialized\n",
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "# model path\n",
    "SKIP_LOAD = False  # Set to True to skip loading models that aren't downloaded yet\n",
    "DEBUG_VIEW = False\n",
    "SKEP_LOAD = False\n",
    "LOAD_FLAG = True\n",
    "DEFAULT_INPUT_IMAGES = 4\n",
    "MAX_INPUT_IMAGES = 4\n",
    "SIZE = 768\n",
    "with_lora = False\n",
    "enable_encoder = False\n",
    "with_pos_mask = True\n",
    "\n",
    "weight_path = f'{package_dir}/cache/flashface.ckpt'\n",
    "\n",
    "# Detect available device\n",
    "gpu = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {gpu}\")\n",
    "\n",
    "# Use enhanced transforms for better image quality\n",
    "padding_to_square = EnhancedPadToSquare(224, padding_mode='constant', fill=0)\n",
    "\n",
    "# Use enhanced transforms for retinaface with better anti-aliasing\n",
    "retinaface_transforms = T.Compose([\n",
    "    EnhancedPadToSquare(size=640, padding_mode='constant', fill=0),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "from ldm.models.retinaface import retinaface, crop_face\n",
    "\n",
    "retinaface = retinaface(pretrained=True,\n",
    "                        device=gpu).eval().requires_grad_(False)\n",
    "\n",
    "# Initialize face transforms (needed for the generate function)\n",
    "face_transforms = create_face_transforms(size=224)\n",
    "\n",
    "\n",
    "def detect_face(imgs=None):\n",
    "    # read images\n",
    "    pil_imgs = imgs\n",
    "    b = len(pil_imgs)\n",
    "    vis_pil_imgs = copy.deepcopy(pil_imgs)\n",
    "\n",
    "    # detection\n",
    "    imgs = torch.stack([retinaface_transforms(u) for u in pil_imgs]).to(gpu)\n",
    "    boxes, kpts = retinaface.detect(imgs, min_thr=0.6)\n",
    "\n",
    "    # undo padding and scaling\n",
    "    face_imgs = []\n",
    "\n",
    "    for i in range(b):\n",
    "        # params\n",
    "        scale = 640 / max(pil_imgs[i].size)\n",
    "        left, top, _, _ = get_padding(round(scale * pil_imgs[i].width),\n",
    "                                      round(scale * pil_imgs[i].height), 640)\n",
    "\n",
    "        # undo padding\n",
    "        boxes[i][:, [0, 2]] -= left\n",
    "        boxes[i][:, [1, 3]] -= top\n",
    "        kpts[i][:, :, 0] -= left\n",
    "        kpts[i][:, :, 1] -= top\n",
    "\n",
    "        # undo scaling\n",
    "        boxes[i][:, :4] /= scale\n",
    "        kpts[i][:, :, :2] /= scale\n",
    "\n",
    "        # crop faces\n",
    "        crops = crop_face(pil_imgs[i], boxes[i], kpts[i])\n",
    "        if len(crops) != 1:\n",
    "            raise ValueError(\n",
    "                f'Warning: {len(crops)} faces detected in image {i}')\n",
    "\n",
    "        face_imgs += crops\n",
    "\n",
    "        # draw boxes on the pil image\n",
    "        draw = ImageDraw.Draw(vis_pil_imgs[i])\n",
    "        for box in boxes[i]:\n",
    "            box = box[:4].tolist()\n",
    "            box = [int(x) for x in box]\n",
    "            draw.rectangle(box, outline='red', width=4)\n",
    "\n",
    "    return face_imgs\n",
    "\n",
    "\n",
    "# Wrapper function to handle reference_faces list\n",
    "def generate_with_faces(pos_prompt, neg_prompt=None, steps=30, face_bbox=[0.3, 0.1, 0.6, 0.4],\n",
    "                       lamda_feat=1.2, face_guidence=3.2, num_sample=1, text_control_scale=7.5,\n",
    "                       seed=-1, step_to_launch_face_guidence=750, reference_faces=None,\n",
    "                       need_detect=True, lamda_feat_before_ref_guidence=0.85):\n",
    "    # Convert reference_faces list to individual parameters\n",
    "    ref_face_1 = reference_faces[0] if reference_faces and len(reference_faces) > 0 else None\n",
    "    ref_face_2 = reference_faces[1] if reference_faces and len(reference_faces) > 1 else None\n",
    "    ref_face_3 = reference_faces[2] if reference_faces and len(reference_faces) > 2 else None\n",
    "    ref_face_4 = reference_faces[3] if reference_faces and len(reference_faces) > 3 else None\n",
    "\n",
    "    return generate(pos_prompt=pos_prompt, neg_prompt=neg_prompt, steps=steps,\n",
    "                   face_bbox=face_bbox, lamda_feat=lamda_feat, face_guidence=face_guidence,\n",
    "                   num_sample=num_sample, text_control_scale=text_control_scale, seed=seed,\n",
    "                   step_to_launch_face_guidence=step_to_launch_face_guidence,\n",
    "                   reference_face_1=ref_face_1, reference_face_2=ref_face_2,\n",
    "                   reference_face_3=ref_face_3, reference_face_4=ref_face_4,\n",
    "                   need_detect=need_detect, lamda_feat_before_ref_guidence=lamda_feat_before_ref_guidence,\n",
    "                   clip_model=clip, clip_tokenizer=clip_tokenizer, \n",
    "                   autoencoder_model=autoencoder, unet_model=unet, diffusion_model=diffusion)\n",
    "\n",
    "\n",
    "if not DEBUG_VIEW and not SKEP_LOAD and not SKIP_LOAD:\n",
    "    clip_tokenizer = data.CLIPTokenizer(padding='eos')\n",
    "    clip = getattr(models, cfg.clip_model)(\n",
    "        pretrained=True).eval().requires_grad_(False).textual.to(gpu)\n",
    "    autoencoder = sd_v1_vae(\n",
    "        pretrained=True).eval().requires_grad_(False).to(gpu)\n",
    "\n",
    "    unet = sd_v1_ref_unet(pretrained=True,\n",
    "                          version='sd-v1-5_nonema',\n",
    "                          enable_encoder=enable_encoder).to(gpu)\n",
    "\n",
    "    unet.replace_input_conv()\n",
    "    unet = unet.eval().requires_grad_(False).to(gpu)\n",
    "    unet.share_cache['num_pairs'] = cfg.num_pairs\n",
    "\n",
    "    if LOAD_FLAG:\n",
    "        model_weight = load_model_weights(weight_path, device='cpu')\n",
    "        msg = unet.load_state_dict(model_weight, strict=True)\n",
    "        print(msg)\n",
    "\n",
    "    # diffusion\n",
    "    sigmas = ops.noise_schedule(schedule=cfg.schedule,\n",
    "                                n=cfg.num_timesteps,\n",
    "                                beta_min=cfg.scale_min,\n",
    "                                beta_max=cfg.scale_max)\n",
    "    diffusion = ContextGaussianDiffusion(sigmas=sigmas,\n",
    "                                         prediction_type=cfg.prediction_type)\n",
    "    diffusion.num_pairs = cfg.num_pairs\n",
    "    print(\"model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import demo_gradio\n",
    "import flashface.all_finetune.ops.context_diffusion\n",
    "importlib.reload(flashface.all_finetune.ops.context_diffusion)\n",
    "importlib.reload(demo_gradio)\n",
    "from demo_gradio import generate, encode_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "final pos_prompt:  A beautiful young asian woman, in a traditional chinese outfit, long hair, complete with a classic hairpin, on the street , white skin, soft light, best quality, masterpiece,ultra-detailed, UHD 4K, photographic\n",
      "final neg_prompt:  blurry, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face\n",
      "detected 3 faces\n",
      "[0.3, 0.2, 0.6, 0.5]\n",
      "detected 3 faces\n",
      "[0.3, 0.2, 0.6, 0.5]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RefStableUNet._patch_attention_forward.<locals>.patched_forward() takes from 2 to 4 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     14\u001b[39m default_text_control_scale = \u001b[32m7.5\u001b[39m\n\u001b[32m     16\u001b[39m default_seed = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m imgs = \u001b[43mgenerate_with_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mneg_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mface_bbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_bbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlamda_feat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlamda_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mface_guidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_guidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtext_control_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_text_control_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mstep_to_launch_face_guidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_to_launch_face_guidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mreference_faces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_imgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mneed_detect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_detect\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# show the generated images\u001b[39;00m\n\u001b[32m     35\u001b[39m img_size = imgs[\u001b[32m0\u001b[39m].size\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mgenerate_with_faces\u001b[39m\u001b[34m(pos_prompt, neg_prompt, steps, face_bbox, lamda_feat, face_guidence, num_sample, text_control_scale, seed, step_to_launch_face_guidence, reference_faces, need_detect, lamda_feat_before_ref_guidence)\u001b[39m\n\u001b[32m     92\u001b[39m ref_face_3 = reference_faces[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m reference_faces \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reference_faces) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     93\u001b[39m ref_face_4 = reference_faces[\u001b[32m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m reference_faces \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reference_faces) > \u001b[32m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneg_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m               \u001b[49m\u001b[43mface_bbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda_feat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlamda_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_guidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_guidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m               \u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_control_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_control_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstep_to_launch_face_guidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_to_launch_face_guidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m               \u001b[49m\u001b[43mreference_face_1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_face_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_face_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_face_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m               \u001b[49m\u001b[43mreference_face_3\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_face_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_face_4\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_face_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m               \u001b[49m\u001b[43mneed_detect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_detect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda_feat_before_ref_guidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlamda_feat_before_ref_guidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m               \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m               \u001b[49m\u001b[43mautoencoder_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\flashface\\all_finetune\\demo_gradio.py:310\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(pos_prompt, neg_prompt, steps, face_bbox, lamda_feat, face_guidence, num_sample, text_control_scale, seed, step_to_launch_face_guidence, reference_face_1, reference_face_2, reference_face_3, reference_face_4, default_pos_prompt, default_neg_prompt, need_detect, lamda_feat_before_ref_guidence, progress, clip_model, clip_tokenizer, autoencoder_model, unet_model, diffusion_model)\u001b[39m\n\u001b[32m    296\u001b[39m     noise = torch.empty(num_sample,\n\u001b[32m    297\u001b[39m                        \u001b[32m4\u001b[39m,\n\u001b[32m    298\u001b[39m                        \u001b[32m768\u001b[39m // \u001b[32m8\u001b[39m,\n\u001b[32m    299\u001b[39m                        \u001b[32m768\u001b[39m // \u001b[32m8\u001b[39m,\n\u001b[32m    300\u001b[39m                        device=gpu).normal_()\n\u001b[32m    301\u001b[39m     config = SampleConfig(\n\u001b[32m    302\u001b[39m         model_kwargs=[c, nc],\n\u001b[32m    303\u001b[39m         guide_scale=text_control_scale,\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m         show_progress=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     z0 = \u001b[43mdiffusion_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43munet_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguide_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguide_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguide_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguide_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m imgs = autoencoder_model.decode(z0 / cfg.ae_scale)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m unet_model.share_cache[\u001b[33m'\u001b[39m\u001b[33mori_similarity\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\flashface\\all_finetune\\ops\\context_diffusion.py:361\u001b[39m, in \u001b[36mContextGaussianDiffusion.sample\u001b[39m\u001b[34m(self, shape, model, model_kwargs, device, guide_scale, guide_rescale, clamp, percentile, clip_denoised, repeat_noise, seed)\u001b[39m\n\u001b[32m    358\u001b[39m     noise = noise[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m].repeat(shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# Apply denoise step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m mu, _, log_var, _, _ = \u001b[43mContextGaussianDiffusion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguide_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguide_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguide_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguide_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclamp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# Compute the next step\u001b[39;00m\n\u001b[32m    375\u001b[39m xt = mu + (\u001b[32m0.5\u001b[39m * log_var).exp() * noise\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\flashface\\all_finetune\\ops\\context_diffusion.py:221\u001b[39m, in \u001b[36mContextGaussianDiffusion.denoise\u001b[39m\u001b[34m(self, xt, t, s, model, model_kwargs, guide_scale, guide_rescale, clamp, percentile)\u001b[39m\n\u001b[32m    218\u001b[39m text_embed = torch.cat([conditional_embed, non_conditional_embed], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m y_out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_xt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m y_out_with_text, y_out_no_text = torch.split(\n\u001b[32m    223\u001b[39m     y_out, y_out.size(\u001b[32m0\u001b[39m) // \u001b[32m2\u001b[39m, dim=\u001b[32m0\u001b[39m\n\u001b[32m    224\u001b[39m )\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# Apply guidance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\flashface\\all_finetune\\models\\reference_unet.py:414\u001b[39m, in \u001b[36mRefStableUNet.forward\u001b[39m\u001b[34m(self, x, t, y, context, mask, caching, style_fidelity)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# encoder-decoder for reference\u001b[39;00m\n\u001b[32m    413\u001b[39m args = (ref_e, ref_context, mask, caching, style_fidelity)\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m ref_x, *xs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;28mself\u001b[39m.decode(ref_x, *args, *xs)\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Process with reference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\ldm\\models\\unet.py:456\u001b[39m, in \u001b[36mUNet.encode\u001b[39m\u001b[34m(self, x, e, context, mask, caching, style_fidelity)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.middle:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (x, ) + \u001b[38;5;28mtuple\u001b[39m(xs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\ldm\\models\\unet.py:487\u001b[39m, in \u001b[36mUNet._forward_single\u001b[39m\u001b[34m(self, module, x, e, context, mask, caching, style_fidelity, reference)\u001b[39m\n\u001b[32m    485\u001b[39m     x = module(x, e)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, StackedAttentionBlocks):\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     x = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_fidelity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn.Sequential) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    489\u001b[39m         module[\u001b[32m0\u001b[39m], Upsample):\n\u001b[32m    490\u001b[39m     x = module[\u001b[32m0\u001b[39m](x, reference)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\ldm\\models\\unet.py:274\u001b[39m, in \u001b[36mStackedAttentionBlocks.forward\u001b[39m\u001b[34m(self, x, context, mask, caching, style_fidelity)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# blocks\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_fidelity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# output\u001b[39;00m\n\u001b[32m    277\u001b[39m x = x.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).view(b, c, h, w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\dev\\ComfyUI-FlashFace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: RefStableUNet._patch_attention_forward.<locals>.patched_forward() takes from 2 to 4 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "# Recommended hyper-parameters to obtain stable ID Fidelity\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/age/{i+1}.png\").convert(\"RGB\") for i in range(3)]\n",
    "need_detect = True\n",
    "pos_prompt = 'A beautiful young asian woman, in a traditional chinese outfit, long hair, complete with a classic hairpin, on the street , white skin, soft light'\n",
    "num_samples = 4\n",
    "# center face position\n",
    "face_bbox =[0.3, 0.2, 0.6, 0.5] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 1.2\n",
    "face_guidence = 0.0  # Temporarily disable face guidance to test basic generation\n",
    "step_to_launch_face_guidence = 750\n",
    "\n",
    "steps = 25\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=None, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended hyper-parameters to obtain stable ID Fidelity\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/age/{i+1}.png\").convert(\"RGB\") for i in range(3)]\n",
    "need_detect = True\n",
    "pos_prompt = 'A very  old woman with short wavy hair'\n",
    "num_samples = 4\n",
    "# center face position\n",
    "face_bbox =[0.3, 0.1, 0.6, 0.4] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 1\n",
    "face_guidence = 2.5\n",
    "step_to_launch_face_guidence = 750\n",
    "\n",
    "steps = 25\n",
    "default_text_control_scale = 8.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=None, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_imgs = [\n",
    "    Image.open(f\"{package_dir}/example_workflows/age/{i+1}.png\").convert(\"RGB\") for i in range(3)\n",
    "]\n",
    "need_detect = True\n",
    "\n",
    "pos_prompt = \"\"\"The cute, beautiful baby girl with medium length brown hair and  pink bow, in the studio \"\"\"\n",
    "# remove beard\n",
    "neg_prompt = None\n",
    "# No face position\n",
    "face_bbox = [0.3, 0.2, 0.6, 0.6]\n",
    "\n",
    "\n",
    "# bigger these three parameters leads to more fidelity but less diversity\n",
    "lamda_feat = 1.2\n",
    "face_guidence = 2\n",
    "step_to_launch_face_guidence = 700\n",
    "\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(\n",
    "    pos_prompt=pos_prompt,\n",
    "    neg_prompt=neg_prompt,\n",
    "    steps=steps,\n",
    "    face_bbox=face_bbox,\n",
    "    lamda_feat=lamda_feat,\n",
    "    face_guidence=face_guidence,\n",
    "    num_sample=4,\n",
    "    text_control_scale=default_text_control_scale,\n",
    "    seed=default_seed,\n",
    "    step_to_launch_face_guidence=step_to_launch_face_guidence,\n",
    "    reference_faces=face_imgs,\n",
    "    need_detect=need_detect,\n",
    ")\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new(\"RGB\", (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio))\n",
    "    )\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/avatar.png\").convert(\"RGB\")]\n",
    "need_detect = True\n",
    "pos_prompt = \"A handsome young man with long brown hair is sitting in the desert\"\n",
    "num_samples = 2\n",
    "# No face position\n",
    "face_bbox =[0., 0., 0., 0.] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 0.9\n",
    "face_guidence = 2.5\n",
    "step_to_launch_face_guidence = 700\n",
    "\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=None, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/snow_white.png\").convert(\"RGB\")]\n",
    "need_detect = True\n",
    "pos_prompt = \"Full body photo of a beautiful young women sitting in the office, medium length wavy hair, wearinig red bow hairpin on the top of head\"\n",
    "num_samples = 2\n",
    "# No face position\n",
    "face_bbox =[0., 0., 0., 0.] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 1\n",
    "face_guidence = 2\n",
    "step_to_launch_face_guidence = 600\n",
    "\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=None, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eren_jaeger.png\n",
    "\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/eren_jaeger.png\").convert(\"RGB\")]\n",
    "need_detect = True\n",
    "pos_prompt =  \"A handsome, attractive, sleek young man sitting on the beach, wearing black long trench coat, man bun hair,  heavily clouded, sunset, sea in the background\"\n",
    "# remove beard\n",
    "neg_prompt = \"beard\"\n",
    "# No face position\n",
    "face_bbox =[0., 0., 0., 0.] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 0.9\n",
    "face_guidence = 2\n",
    "step_to_launch_face_guidence = 600\n",
    "num_samples = 2\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=neg_prompt, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary people\n",
    "\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/man_face/{i+1}.png\").convert(\"RGB\") for i in range(4)]\n",
    "need_detect = True\n",
    "pos_prompt =  \"An handsome young man, with cowboy hat, long hair, full body, standing in the forest, sunset\"\n",
    "# remove beard\n",
    "neg_prompt = \"beard\"\n",
    "# No face position\n",
    "face_bbox =[0., 0., 0., 0.] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 0.85\n",
    "face_guidence = 2\n",
    "step_to_launch_face_guidence = 600\n",
    "num_samples = 2\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=neg_prompt, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples,\n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary people\n",
    "\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/woman_face/{i+1}.png\").convert(\"RGB\") for i in range(4)]\n",
    "need_detect = True\n",
    "pos_prompt =  'A beautiful young woman with short curly hair in the garden holding a flower'\n",
    "# remove beard\n",
    "neg_prompt = None\n",
    "# No face position\n",
    "face_bbox =[0., 0., 0., 0.] \n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "lamda_feat = 1\n",
    "face_guidence = 2.3\n",
    "step_to_launch_face_guidence = 600\n",
    "num_samples = 2\n",
    "steps = 50\n",
    "default_text_control_scale = 7.5\n",
    "\n",
    "default_seed = 0\n",
    "\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=neg_prompt, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details\n",
    "\n",
    "\n",
    "face_imgs = [Image.open(f\"{package_dir}/example_workflows/details_face/{i+1}.jpeg\").convert(\"RGB\") for i in range(4)]\n",
    "need_detect = True\n",
    "pos_prompt =  'A beautiful young woman stands in the street,  wearing earing and white skirt and  hat, thin body, sunny day'\n",
    "# remove beard\n",
    "neg_prompt = 'Bangs'\n",
    "# left top corner\n",
    "face_bbox =  [0.1, 0.1, 0.5, 0.5]\n",
    "# bigger these three parameters leads to more fidelity but less diversity \n",
    "\n",
    "lamda_feat = 1.3\n",
    "face_guidence = 3.2\n",
    "step_to_launch_face_guidence = 800\n",
    "\n",
    "steps = 50\n",
    "default_text_control_scale = 8\n",
    "\n",
    "default_seed = 0\n",
    "num_samples = 2\n",
    "\n",
    "imgs = generate_with_faces(pos_prompt=pos_prompt, \n",
    "                    neg_prompt=neg_prompt, \n",
    "                    steps=steps, \n",
    "                    face_bbox=face_bbox,\n",
    "                    lamda_feat=lamda_feat, \n",
    "                    face_guidence=face_guidence, \n",
    "                    num_sample=num_samples, \n",
    "                    text_control_scale=default_text_control_scale, \n",
    "                    seed=default_seed, \n",
    "                    step_to_launch_face_guidence=step_to_launch_face_guidence, \n",
    "                    reference_faces=face_imgs,\n",
    "                    need_detect=need_detect\n",
    "                    )\n",
    "\n",
    "\n",
    "# show the generated images\n",
    "img_size = imgs[0].size\n",
    "num_imgs = len(imgs)\n",
    "save_img = Image.new('RGB', (img_size[0] * (num_imgs + 1), img_size[1]))\n",
    "for i, img in enumerate(imgs):\n",
    "    save_img.paste(img, ((i + 1) * img_size[0], 0))\n",
    "\n",
    "# paste all four reference face imgs to the first\n",
    "\n",
    "resize_w = img_size[0] // 2\n",
    "resize_h = img_size[1] // 2\n",
    "\n",
    "for id, ref_img in enumerate(face_imgs):\n",
    "    # resize the ref_img keep the ratio to fit the size of (resize_w, resize_h)\n",
    "    w_ratio = resize_w / ref_img.size[0]\n",
    "    h_ratio = resize_h / ref_img.size[1]\n",
    "    ratio = min(w_ratio, h_ratio)\n",
    "    ref_img = ref_img.resize(\n",
    "        (int(ref_img.size[0] * ratio), int(ref_img.size[1] * ratio)))\n",
    "\n",
    "    if id < 2:\n",
    "        save_img.paste(ref_img, (id * resize_w, 0))\n",
    "    else:\n",
    "        save_img.paste(ref_img, ((id - 2) * resize_w, resize_h))\n",
    "\n",
    "display(save_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
